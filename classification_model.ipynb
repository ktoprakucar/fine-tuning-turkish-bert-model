{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "classification_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNbCmdvsuK6pRgtr6rIiBQa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ktoprakucar/fine-tuning-turkish-bert-model/blob/master/classification_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgcx4jPKRd8V"
      },
      "source": [
        "koda geçmeden önce, koddaki BERT ile ilgili olan kısımları https://mccormickml.com/2019/07/22/BERT-fine-tuning/ linkindeki kodlardan yararlanarak oluşturduğumu belirtmek isterim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrYO6HDoRNNP",
        "outputId": "186b0f6e-4d7c-4320-e91c-2bb94b8d6240",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "!pip install transformers\n",
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCCSYp9sSlSD",
        "outputId": "1bcb89d1-2a2c-4fd5-aac1-15b529a9894a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# check GPU\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name == '/device:GPU:0':\n",
        "    device = torch.device(\"cuda\")\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla K80\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dglYrRbCbR8Y"
      },
      "source": [
        "dosyayı drive'dan okuyabilmeniz için google drive'daki ana klasörünüze resource isminde bir klasör oluşturup, kaggle'dan indirdiğimiz dosyayı *turkish_text_data*.csv olarak kaydetmemiz gerekiyor.\n",
        "\n",
        "kaggle data'sını linkten indirebilirsiniz: https://www.kaggle.com/savasy/ttc4900"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tc-DVN1S448",
        "outputId": "ffee0f78-3fc7-4b15-caff-f0c707e87b89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# set environment as googledrive to folder \"resource\"\n",
        "data_path =  \"/resource/\"\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    data_path = \"/content/drive/My Drive/resource/\"\n",
        "\n",
        "except:\n",
        "    print(\"You are not working in Colab at the moment :(\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Nt6KR1ZTMEh"
      },
      "source": [
        "df = pd.read_csv(data_path + 'turkish_text_data.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaWQQe3yTYab",
        "outputId": "f8435af4-b9d0-47f0-b6fb-8b5a5ba06f10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 4900 entries, 0 to 4899\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   category  4900 non-null   object\n",
            " 1   text      4900 non-null   object\n",
            "dtypes: object(2)\n",
            "memory usage: 76.7+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOTi-A1UTc24",
        "outputId": "de4917e1-4d2c-4258-c63e-f84b4dc7b530",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-71502ad6-def4-413a-bd77-82ce0ffe75d8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>457</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>erdoğan yeni bir süreç değil başbakan_recep_t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2883</th>\n",
              "      <td>saglik</td>\n",
              "      <td>nefesler tutuldu ikizler ayrıldı abd de göğüs...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1511</th>\n",
              "      <td>ekonomi</td>\n",
              "      <td>asker ve öğrenciyi indirimli uçuracak borajet...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3557</th>\n",
              "      <td>spor</td>\n",
              "      <td>ibb çıkış arıyor turuncu lacivertliler yarınk...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1846</th>\n",
              "      <td>ekonomi</td>\n",
              "      <td>mütekabiliyet arapları coşturdu yabancılara m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>535</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>bahçeli den 29 ekim açıklaması bahçeli cumhur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>açlık grevini ne olur bırakın başbakan_yardım...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2345</th>\n",
              "      <td>kultur</td>\n",
              "      <td>tepenin_ardı nda da biz varız hurriyet com tr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2066</th>\n",
              "      <td>ekonomi</td>\n",
              "      <td>300 firma 60 bin çifti evlendirecek yaklaşık ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>siyaset</td>\n",
              "      <td>gül böyle bir lüksümüz yok cumhurbaşkani abdu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71502ad6-def4-413a-bd77-82ce0ffe75d8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71502ad6-def4-413a-bd77-82ce0ffe75d8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71502ad6-def4-413a-bd77-82ce0ffe75d8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      category                                               text\n",
              "457   siyaset    erdoğan yeni bir süreç değil başbakan_recep_t...\n",
              "2883   saglik    nefesler tutuldu ikizler ayrıldı abd de göğüs...\n",
              "1511  ekonomi    asker ve öğrenciyi indirimli uçuracak borajet...\n",
              "3557     spor    ibb çıkış arıyor turuncu lacivertliler yarınk...\n",
              "1846  ekonomi    mütekabiliyet arapları coşturdu yabancılara m...\n",
              "535   siyaset    bahçeli den 29 ekim açıklaması bahçeli cumhur...\n",
              "451   siyaset    açlık grevini ne olur bırakın başbakan_yardım...\n",
              "2345   kultur    tepenin_ardı nda da biz varız hurriyet com tr...\n",
              "2066  ekonomi    300 firma 60 bin çifti evlendirecek yaklaşık ...\n",
              "495   siyaset    gül böyle bir lüksümüz yok cumhurbaşkani abdu..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEcaS2orThoy",
        "outputId": "c6038b44-b9b5-454d-a488-ac4e6624c18f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.groupby('category').size()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "category\n",
              "dunya         700\n",
              "ekonomi       700\n",
              "kultur        700\n",
              "saglik        700\n",
              "siyaset       700\n",
              "spor          700\n",
              "teknoloji     700\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BW4WCeTb1a5"
      },
      "source": [
        "Kategorik olan label'ları modelde kullanabilmemiz için kategori kolonunu encode etmemiz gerekiyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KOo9dA56_7Oy"
      },
      "source": [
        "df['encoded_categories'] = LabelEncoder().fit_transform(df['category'])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMsdi2PzcAdY"
      },
      "source": [
        "Bert için gerekli olan 2 ana objeyi, tokenizer'ı ve model'i hugging face'ten indirebilirsiniz: https://huggingface.co/models\n",
        "\n",
        "tokenizer'ı, önceden sahip olunan kelime haznesini kullanarak metinini ögelerini ayırma işleminde kullanılan araç olarak tanımlayabiliriz. bu tokenizer'daki kelimelere aşağıdaki linkten ulaşabilirsiniz: \n",
        "https://s3.amazonaws.com/models.huggingface.co/bert/dbmdz/bert-base-turkish-128k-uncased/vocab.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtmbRhroAtCd"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-turkish-128k-uncased', do_lower_case=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DzbC8f7AxAC"
      },
      "source": [
        "sentences = df.text.values"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep3B9i-043xM"
      },
      "source": [
        "metinlerin kelime sayısı 250'den fazla olmasına rağmen girdi uzunluğunu maksimum 250 olarak belirtiyoruz. daha büyük değerlerde GPU'nun memory'si yetmediği için hata alıyoruz. aşağıda göreceğimiz üzere 250 kelimelik metinlerle bile iyi sonuçlar elde edilebiliyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPgQuTUGA1ct"
      },
      "source": [
        "max_len = 200"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCqcXz6scVLD"
      },
      "source": [
        "burada elimizdeki metin verisini %80 ve %20 oranıyla, sırasıyla training ve test olarak ikiye bölüyoruz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsHlvfa1A9J_"
      },
      "source": [
        "training = df.groupby('category').apply(lambda x : x.sample(frac = 0.8))\n",
        "test = pd.concat([df,training]).drop_duplicates(keep=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYiFqjplA_e4",
        "outputId": "bab056a9-459f-48f2-f296-8eb59a77123f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Training: \", len(training))\n",
        "print(\"Test: \", len(test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training:  3920\n",
            "Test:  837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PuTo9fHBBFd"
      },
      "source": [
        "training_texts = training.text.values\n",
        "training_labels = training.encoded_categories.values"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u793WoNkclVP"
      },
      "source": [
        "bu kısımda metin verisini modelde kullanmak üzere işliyoruz. öncelikle cümledeki kelimeler indirdiğimiz tokenizer ile tokenize ediliyor, sonrasında sınıflandırma probleminin çözülebilmesi için gerekli olan token'lar cümlenin sonuna ve başına ekleniyor. cümle maksimum uzunluktan kısaysa, input vektörümüz sabit uzunlukta olduğu için boşluklar dolduruluyor, uzunsa metin limit kadar kelime ile ifade ediliyor. attention mask'leri oluşturuluyor ve metinler işlemin sonucunda tensor objesi olarak geri dönüyor.\n",
        "\n",
        "aşağıdaki çıktıda da görüldüğü üzere, metindeki kelimeler tokenizer'daki kelimelerin id'leri ile ifade ediliyor ve bu şekilde işleme sokuluyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AavMUW7oBESR",
        "outputId": "ff9abcba-40a7-41e8-b551-03675a043b52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in training_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                     \n",
        "                        add_special_tokens = True,\n",
        "                        max_length = max_len,      \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True, \n",
        "                        return_tensors = 'pt',\n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(training_labels)\n",
        "\n",
        "print('Original: ', training_texts[0])\n",
        "print('Token IDs:', input_ids[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   ölü sayısı 158 e yükseldi suriye_insan_hakları_örgütü snhr suriye ordusunun çeşitli kentlerde yönetim karşıtlarına yönelik ağır silahlarla düzenlediği operasyonlarda ölü sayısının 158 e yükseldiğini duyurdu londra merkezli örgüt esed birliklerinin başkent şam ın bazı banliyölerine hava ve tank destekli olarak gerçekleştirdiği operasyonlarda 74 kişinin öldüğünü çok sayıda kişinin yaralandığını bildirdi örgüt kuzeydeki halep kentinde 35 humus ta 14 idlib de 12 hama da 10 deyru z zor da 6 dera kentinde 4 ve lazkiye de 3 olmak üzere ülke genelindeki olaylarda ölü sayısının 158 e çıktığını kaydetti suriye nin farklı bölgelerinde olayları takip eden yerel aktivistler özgür_suriye_ordusu nun öso savunma taktiğini bırakarak taarruza geçtiğini öne sürdü aktivistler idlib in maaret el numan ilçesinin tamamıyla muhaliflerin eline geçtiğini öso nun şam idlib deyru z zor dera halep hama ve humus kentinde ilerlemeler kaydettiğini belirttiler bu arada başkentin güneyinde yer alan kadem aseli nahr aişe ile tadamun bölgesinde nizami birliklerle öso arasında şiddetli çatışmaların yaşandığını aktaran şam haber ağı snn öso un söz konusu bölgelerde düzenli orduya ait 5 tankla birlikte çok sayıda askeri aracı etkisiz hale getirdiğini bildirdi snn özgür suriye ordusunun şam ın zablatani ve doğu guta bölgelerinde birçok askeri noktaya saldırdığını nizami ordunun büyük kayıplar verdiğini kaydetti humus un birçok semtinde de yoğun sokak çatışmalarının meydana geldiğine işaret eden haber ağı kuseyr ve halidiye bölgelerini ele geçirmeye çalışan suriye ordusunun başarısız olduğunu vurguladı haber ağı muhalif tugayların humus ta çok sayıda tank ve askeri aracı etkisiz hale getirerek yüksek miktarda mühimmat ve cephanelik ele geçirdiğini belirtti snn halep tede iki ordu arasında çatışmalar yaşandığını nizami ordunun savaş uçaklarıyla yerleşim yerlerini vurduğunu aktardı aa\n",
            "Token IDs: tensor([     2,  10757,   4431,  34947,     47,  73923,   1971,   4842,     41,\n",
            "          2374,     41,   5766,     41,   8299,   6997,   8394,  17404,   4842,\n",
            "         16542, 106483,  31828, 101254,  40209,  32312,   2061,  13286,   2373,\n",
            "         84100,  31034,  54080,   5288,   2242,  25715,  10757,   8369,  34947,\n",
            "            47,  73923,  26199,  10233,   7539,  10889,   8299,   2062,  25158,\n",
            "         29268,   5741,   3678,   3681,   2325,   2759,  34194,  11137,  58696,\n",
            "          3022,   1946,   9054,  12878,   2095, 100867,  16222,  25715,   9481,\n",
            "         57643,  15053,   6110,   4928,  57643,   9293,  22328,   7011,   8299,\n",
            "          2062,  43538,  17388,  10966,   4983,  45073,   2058,   3225,   3258,\n",
            "         16193,   1961,   2827,  50876,   1972,   2562,  99587,   1012,     68,\n",
            "          2534,   1972,     26,  88799,  10966,     24,   1946,  74852,   1961,\n",
            "            23,   2682,  67408,  87717,  24487,  24940,  10757,   8369,  34947,\n",
            "            47,  73056,  22328,   5703,   4842,   2276,   2693,  75575,   4260,\n",
            "          9107,   3505,   2698,   4756,  93885,  74297,     41,   4842,     41,\n",
            "          9203,   3095,   3372,   1008,   5406,   2311,  82328,  14381,  78345,\n",
            "         60013,   7426,  12218,   3241,   2000,  93885,   3258,  16193,   2031,\n",
            "         96326,   1949,   2210,  28243,   2068,  93845,  19849,  41537,   8595,\n",
            "         60013,   7426,   3372,   1008,   3095,   3681,   3258,  16193,  99587,\n",
            "          1012,     68,   2534,  88799,  17388,  50876,   1946,  45073,  10966,\n",
            "         45594,   5703,   7426,  23687,   1964,   4109,   5741,  30814, 105867,\n",
            "          2007,   2101,   2309,  13238,  83631,   1987,  17071,   1018,  53500,\n",
            "          1011,   2037,  28512,  11704,  75575,   2386,  72295, 121290,   3372,\n",
            "          1008,      3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4YUhZxyCwVF"
      },
      "source": [
        "train_dataset = TensorDataset(input_ids, attention_masks, labels)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYQJJAr1e_a4"
      },
      "source": [
        "oluşturduğumuz tensor verisini modele vermek üzere *dataloader* değişkenine dönüştürüyoruz. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTUsi4QGBxQr"
      },
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,  \n",
        "            sampler = RandomSampler(train_dataset), \n",
        "            batch_size = batch_size \n",
        "        )"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxa9wOcVCzq8"
      },
      "source": [
        "number_of_categories = len(df['encoded_categories'].unique())"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zow9XGXVfNhS"
      },
      "source": [
        "tokenizer'da olduğu gibi, önceden train edilmiş olan modeli fine tune etmek için hugging face'ten indiriyoruz. modelin özelliklerine aşağıdaki linkten ulaşabilirsiniz: \n",
        "https://s3.amazonaws.com/models.huggingface.co/bert/dbmdz/bert-base-turkish-128k-uncased/config.json\n",
        "\n",
        "en altta *model.cuda()* metotu ile modelin GPU'da kullanılacağını belirtiyoruz. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfNDxpE3C2bP",
        "outputId": "e28d46b2-ce43-4591-9af7-50313dff535c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"dbmdz/bert-base-turkish-128k-uncased\",\n",
        "    num_labels = number_of_categories, \n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "model.cuda()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dbmdz/bert-base-turkish-128k-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-128k-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(128000, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=7, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoEqn64XfjX3"
      },
      "source": [
        "training'den önceki son adımda, toplam training adım sayısını ve kaç kere training yapılacağı sayısını belirliyoruz. bu sayıların yanında, öğrenmenin daha verimli olabilmesi ve *learning rate* optimizasyonu için bir scheduler yaratılıyor ve optimizer olarak *Adam Optimizer* kullanılıyor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPL4qSPkC8hx"
      },
      "source": [
        "epochs = 4\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5,\n",
        "                  eps = 1e-8 \n",
        "                )\n",
        "\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqFHIKBhE9wv"
      },
      "source": [
        "def format_time(elapsed):\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u239XcAe4Uq9"
      },
      "source": [
        "training aşamasına geçmeden önce seed değerini sabit bir değere eşitliyoruz ki, bütün deneylerimizde aynı sonucu alabilelim.\n",
        "\n",
        "training, toplam bölüm (epoch) sayısı kadar, bizde 4, kez yapılıyor. yukarıda training verisetini dataloader'a aktarmıştık, girdileri 32'şer 32'şer alıp modeli besliyoruz ve training başlıyor. her bölüm başlamadan önce optimize edilecek loss değeri sıfırlanıyor. modelin *train()* metotu çağırılıyor. çünkü test kısmında *eval()* metotu çağırılıyor. modelin katmanları train ve eval metotlarında farklı olarak davranıyor. dataloader'daki değerler GPU'ya aktarılıyor, gradient değerleri sıfırlanıyor ve output (logit) değerleri oluşuyor ve buna bağlı olarak loss değeri hesaplanıyor. backpropogation ile gradient'ler tekrar hesaplanıyor ve son olarak da learnig rate'le beraber parametreler de optimize ediliyor. her bölümün sonunda ortalama loss'u inceleyebiliriz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or7cXZA9DPs4",
        "outputId": "04bbfd5c-a71c-4aa1-cb41-0d5932926b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "seed_val = 1903\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "training_stats = []\n",
        "total_t0 = time.time()\n",
        "\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    t0 = time.time()\n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 10 == 0 and not step == 0:\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            print('Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "        output = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        loss = output['loss']\n",
        "        logits = output['logits']\n",
        "        total_train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Training Time': training_time,\n",
        "        }\n",
        "    )\n",
        "\n",
        "print(\"Training completed in {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======== Epoch 1 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:23.\n",
            "Batch    20  of    123.    Elapsed: 0:00:45.\n",
            "Batch    30  of    123.    Elapsed: 0:01:07.\n",
            "Batch    40  of    123.    Elapsed: 0:01:30.\n",
            "Batch    50  of    123.    Elapsed: 0:01:52.\n",
            "Batch    60  of    123.    Elapsed: 0:02:15.\n",
            "Batch    70  of    123.    Elapsed: 0:02:37.\n",
            "Batch    80  of    123.    Elapsed: 0:02:59.\n",
            "Batch    90  of    123.    Elapsed: 0:03:22.\n",
            "Batch   100  of    123.    Elapsed: 0:03:44.\n",
            "Batch   110  of    123.    Elapsed: 0:04:06.\n",
            "Batch   120  of    123.    Elapsed: 0:04:29.\n",
            "Average training loss: 0.50\n",
            "Training epoch took: 0:04:34\n",
            "======== Epoch 2 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:22.\n",
            "Batch    20  of    123.    Elapsed: 0:00:45.\n",
            "Batch    30  of    123.    Elapsed: 0:01:07.\n",
            "Batch    40  of    123.    Elapsed: 0:01:29.\n",
            "Batch    50  of    123.    Elapsed: 0:01:52.\n",
            "Batch    60  of    123.    Elapsed: 0:02:14.\n",
            "Batch    70  of    123.    Elapsed: 0:02:36.\n",
            "Batch    80  of    123.    Elapsed: 0:02:59.\n",
            "Batch    90  of    123.    Elapsed: 0:03:21.\n",
            "Batch   100  of    123.    Elapsed: 0:03:43.\n",
            "Batch   110  of    123.    Elapsed: 0:04:06.\n",
            "Batch   120  of    123.    Elapsed: 0:04:28.\n",
            "Average training loss: 0.16\n",
            "Training epoch took: 0:04:34\n",
            "======== Epoch 3 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:22.\n",
            "Batch    20  of    123.    Elapsed: 0:00:45.\n",
            "Batch    30  of    123.    Elapsed: 0:01:07.\n",
            "Batch    40  of    123.    Elapsed: 0:01:29.\n",
            "Batch    50  of    123.    Elapsed: 0:01:52.\n",
            "Batch    60  of    123.    Elapsed: 0:02:14.\n",
            "Batch    70  of    123.    Elapsed: 0:02:36.\n",
            "Batch    80  of    123.    Elapsed: 0:02:59.\n",
            "Batch    90  of    123.    Elapsed: 0:03:21.\n",
            "Batch   100  of    123.    Elapsed: 0:03:44.\n",
            "Batch   110  of    123.    Elapsed: 0:04:06.\n",
            "Batch   120  of    123.    Elapsed: 0:04:28.\n",
            "Average training loss: 0.07\n",
            "Training epoch took: 0:04:34\n",
            "======== Epoch 4 / 4 ========\n",
            "Batch    10  of    123.    Elapsed: 0:00:22.\n",
            "Batch    20  of    123.    Elapsed: 0:00:45.\n",
            "Batch    30  of    123.    Elapsed: 0:01:07.\n",
            "Batch    40  of    123.    Elapsed: 0:01:29.\n",
            "Batch    50  of    123.    Elapsed: 0:01:52.\n",
            "Batch    60  of    123.    Elapsed: 0:02:14.\n",
            "Batch    70  of    123.    Elapsed: 0:02:37.\n",
            "Batch    80  of    123.    Elapsed: 0:02:59.\n",
            "Batch    90  of    123.    Elapsed: 0:03:21.\n",
            "Batch   100  of    123.    Elapsed: 0:03:44.\n",
            "Batch   110  of    123.    Elapsed: 0:04:06.\n",
            "Batch   120  of    123.    Elapsed: 0:04:28.\n",
            "Average training loss: 0.03\n",
            "Training epoch took: 0:04:34\n",
            "Training completed in 0:18:16 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGlqNzzN3f0K"
      },
      "source": [
        "training'deki model performansı incelemek için loss'daki düşüşü inceliyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEX93h5OE-pG",
        "outputId": "7a6bbf03-b6cf-4ebf-8e1f-a4402318ac0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "df_stats = pd.DataFrame(data=training_stats)\n",
        "plt.plot(df_stats['Training Loss'], label=\"Training\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ6RTYEu3nwL"
      },
      "source": [
        "training verisetinde olduğu gibi, test veriseti için de bir dataloader oluşturuyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xa3aIdMyJwm-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80fefa83-3a1d-4fa8-8eac-a533f2fa9139"
      },
      "source": [
        "test_texts = test.text.values\n",
        "test_labels = test.encoded_categories.values\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "\n",
        "for text in test_texts:\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        text,                     \n",
        "                        add_special_tokens = True, \n",
        "                        max_length = max_len,          \n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,  \n",
        "                        return_tensors = 'pt',   \n",
        "                   )\n",
        "    \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "input_ids = torch.cat(input_ids, dim=0)\n",
        "attention_masks = torch.cat(attention_masks, dim=0)\n",
        "labels = torch.tensor(test_labels)\n",
        "\n",
        "batch_size = 32  \n",
        "\n",
        "prediction_data = TensorDataset(input_ids, attention_masks, labels)\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxGSl8Efe8JM"
      },
      "source": [
        "test verisini kullanarak modele sonuçları tahmin ettiriyoruz. batch değerimiz 32 olduğu için, model training'de olduğu gibi prediction kısmında da 32'şer 32'şer input'ları modele veriyor. o yüzden flatten fonksiyonu ile bütün sonuçları tek bir listede topluyoruz ve prediction_set değişkeninde saklıyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RNfzOIZKH82",
        "outputId": "b94d4a7c-7db2-4372-f54a-bbf2e39d0601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('Prediction started on test data')\n",
        "model.eval()\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for batch in prediction_dataloader:\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "  with torch.no_grad():\n",
        "      outputs = model(b_input_ids, token_type_ids=None, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('Prediction completed')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction started on test data\n",
            "Prediction completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNJdxER7KeBH"
      },
      "source": [
        "prediction_set = []\n",
        "\n",
        "for i in range(len(true_labels)):\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  prediction_set.append(pred_labels_i)\n",
        "\n",
        "prediction_scores = [item for sublist in prediction_set for item in sublist]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZXQhPf8fCdb"
      },
      "source": [
        "bu bir sınıflandırma problemi olduğu için performans metriklerinden F-score'u kullanmak istedim. bu kısımda Precision, Recall ve F-score değerlerini çıkartıyoruz, modelin performansını gözlemliyoruz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tk4NmFh0KjLu"
      },
      "source": [
        "f_score = f1_score(test_labels, prediction_scores, average='macro')\n",
        "precision = precision_score(test_labels, prediction_scores, average='macro')\n",
        "recall = recall_score(test_labels, prediction_scores, average='macro')"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kF8xMzqe3wqV"
      },
      "source": [
        "görüldüğü üzere, kısıtlı bir veriseti ile bile iyi bir performans edilebiliyor. verisetinin kısıtlı olmasının dışında, cümlelerde 250'den fazla kelime olmasına rağmen, input layer kısmında her cümle 250 kelimeyle ifade edilip yüksek F-score elde edilebildi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQaJueaUKlQf",
        "outputId": "3c0dfd2d-7157-46b5-8b57-ddce6a4e50cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"F-Score: \", f_score)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"Precision: \", precision)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-Score:  0.9299967405612592\n",
            "Recall:  0.93170149857694\n",
            "Precision:  0.928909246064119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTaAxtnyKnzf"
      },
      "source": [
        "report = pd.DataFrame(classification_report(test_labels, prediction_scores, output_dict=True))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyJ5RCD-Kqur"
      },
      "source": [
        "report = report.rename(columns={'0':'dunya',\n",
        "                          '1':'ekonomi',\n",
        "                          '2':'kultur',\n",
        "                          '3':'saglik',\n",
        "                          '4':'siyaset',\n",
        "                          '5':'spor',\n",
        "                          '6':'teknoloji'})"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjp7m2NRMoRp",
        "outputId": "f72c881d-bd6a-460b-a5d1-9a8ead4a18eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "report"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-46e718a6-5ae9-4305-ac32-a9e14c200b64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>dunya</th>\n",
              "      <th>ekonomi</th>\n",
              "      <th>kultur</th>\n",
              "      <th>saglik</th>\n",
              "      <th>siyaset</th>\n",
              "      <th>spor</th>\n",
              "      <th>teknoloji</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro avg</th>\n",
              "      <th>weighted avg</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>precision</th>\n",
              "      <td>0.914062</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.893617</td>\n",
              "      <td>0.929825</td>\n",
              "      <td>0.976562</td>\n",
              "      <td>0.982906</td>\n",
              "      <td>0.908333</td>\n",
              "      <td>0.92951</td>\n",
              "      <td>0.928909</td>\n",
              "      <td>0.930211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recall</th>\n",
              "      <td>0.906977</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.963636</td>\n",
              "      <td>0.905797</td>\n",
              "      <td>0.991379</td>\n",
              "      <td>0.923729</td>\n",
              "      <td>0.92951</td>\n",
              "      <td>0.931701</td>\n",
              "      <td>0.929510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>f1-score</th>\n",
              "      <td>0.910506</td>\n",
              "      <td>0.897059</td>\n",
              "      <td>0.913043</td>\n",
              "      <td>0.946429</td>\n",
              "      <td>0.939850</td>\n",
              "      <td>0.987124</td>\n",
              "      <td>0.915966</td>\n",
              "      <td>0.92951</td>\n",
              "      <td>0.929997</td>\n",
              "      <td>0.929541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>support</th>\n",
              "      <td>129.000000</td>\n",
              "      <td>136.000000</td>\n",
              "      <td>90.000000</td>\n",
              "      <td>110.000000</td>\n",
              "      <td>138.000000</td>\n",
              "      <td>116.000000</td>\n",
              "      <td>118.000000</td>\n",
              "      <td>0.92951</td>\n",
              "      <td>837.000000</td>\n",
              "      <td>837.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-46e718a6-5ae9-4305-ac32-a9e14c200b64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-46e718a6-5ae9-4305-ac32-a9e14c200b64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-46e718a6-5ae9-4305-ac32-a9e14c200b64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                dunya     ekonomi  ...   macro avg  weighted avg\n",
              "precision    0.914062    0.897059  ...    0.928909      0.930211\n",
              "recall       0.906977    0.897059  ...    0.931701      0.929510\n",
              "f1-score     0.910506    0.897059  ...    0.929997      0.929541\n",
              "support    129.000000  136.000000  ...  837.000000    837.000000\n",
              "\n",
              "[4 rows x 10 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    }
  ]
}